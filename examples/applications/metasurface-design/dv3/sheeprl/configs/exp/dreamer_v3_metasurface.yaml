# @package _global_

defaults:
  - override /algo: dreamer_v3_L
  - override /env: metasurface
  - override /model_manager: dreamer_v3
  - _self_

# exp: dreamer_v3_metasurface
seed: 42
# Algorithm
algo:
  replay_ratio: 2
  total_steps: 50000
  horizon: 15
  per_rank_batch_size: 8
  per_rank_sequence_length: 32
  mlp_keys:
    encoder: [struct]
    decoder: [struct]
  cnn_keys:
    encoder: [real_field, imag_field]
    decoder: [real_field, imag_field]
  actor:
    ent_coef: 3e-4

  learning_starts: 1024
  # world_model:
  #   encoder:
  #     cnn_channels_multiplier: 16
  #   observation_model:
  #     cnn_channels_multiplier: 256

env:
  screen_size: 256
  num_envs: 8
  max_episode_steps: 512
  capture_video: True
  grayscale: True
  # vmax: 5.

# Checkpoint
checkpoint:
  every: 1000
  # resume_from: /home/anthony/test-sheeprl/logs/logs/runs/dreamer_v3/MeentIndexEfield-v0/2024-04-22_09-05-01_dreamer_v3_MeentIndexEfield-v0_42/version_0/checkpoint/ckpt_388000_0.ckpt

# Buffer
buffer:
  size: 25000
  checkpoint: False

# Distribution
distribution:
  type: "auto"

fabric:
  devices: 1
  accelerator: cuda

metric:
  log_every: 100
  aggregator:
    metrics:
      Loss/world_model_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/value_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/policy_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/observation_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/reward_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/state_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/continue_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      State/kl:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      State/post_entropy:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      State/prior_entropy:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Params/exploration_amount:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/world_model:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/actor:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/critic:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Env/max_eff:
        _target_: torchmetrics.MaxMetric
        sync_on_compute: ${metric.sync_on_compute}

hydra:
  job:
    env_set:
      OMP_NUM_THREADS: 8